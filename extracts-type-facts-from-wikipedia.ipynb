{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba4043a",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/angevalli/extracts-type-facts-from-wikipedia?scriptVersionId=133920084\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a><a target=\"_blank\" href=\"https://drive.google.com/drive/folders/1Zx90bl8OwIEBK2jCFzyNV4FxvuJ9efSl?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><a target=\"_blank\" href=\"https://drive.google.com/drive/folders/1KgANSekj2YwkT2fpy-89AnmnCDF870UC?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7b1dfc",
   "metadata": {
    "papermill": {
     "duration": 0.004385,
     "end_time": "2023-06-17T15:13:32.916483",
     "exception": false,
     "start_time": "2023-06-17T15:13:32.912098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Extracts type facts from a Wikipedia file\n",
    "\n",
    "\n",
    "=== Purpose ===\n",
    "\n",
    "The goal of this lab is to extract the class to which an entity belongs from Wikipedia.\n",
    "For example, given the Wikipedia article about Leicester:\n",
    "\n",
    "    Leicester is a small city in England\n",
    "\n",
    "the goal is to extract:\n",
    "\n",
    "    Leicester TAB city\n",
    "\n",
    "\n",
    "=== Task ===\n",
    "\n",
    "Complete the extract_type() function so that it extracts the type of the article entity from the content.\n",
    "For example, for a content of \"Leicester is a beautiful English city in the UK\", it should return \"city\".\n",
    "Exclude terms that are too abstract (\"member of...\", \"way of...\"), and try to extract exactly the noun(s).\n",
    "You can also skip articles (e.g. return None) if you are not sure or if the text does not contain any type.\n",
    "In order to ensure a fair evaluation, do not use any non-standard Python libraries except NLTK.\n",
    "\n",
    "Input:\n",
    "April\n",
    "April is the fourth month of the year with 30 days.\n",
    "\n",
    "Output:\n",
    "April TAB month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d3bffe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-17T15:13:32.92669Z",
     "iopub.status.busy": "2023-06-17T15:13:32.925689Z",
     "iopub.status.idle": "2023-06-17T15:13:32.947838Z",
     "shell.execute_reply": "2023-06-17T15:13:32.946877Z"
    },
    "papermill": {
     "duration": 0.030167,
     "end_time": "2023-06-17T15:13:32.950666",
     "exception": false,
     "start_time": "2023-06-17T15:13:32.920499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wikidata/wikipedia-first.txt\n",
      "/kaggle/input/type-facts-in-wikipedia/gold-standard-sample.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a3e26e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T15:13:32.960924Z",
     "iopub.status.busy": "2023-06-17T15:13:32.960511Z",
     "iopub.status.idle": "2023-06-17T15:13:32.98034Z",
     "shell.execute_reply": "2023-06-17T15:13:32.979311Z"
    },
    "papermill": {
     "duration": 0.028064,
     "end_time": "2023-06-17T15:13:32.982942",
     "exception": false,
     "start_time": "2023-06-17T15:13:32.954878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utility_script_entity_disambiguation_and_typing import Page, Parsy, cal_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a001d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T15:13:32.994594Z",
     "iopub.status.busy": "2023-06-17T15:13:32.993354Z",
     "iopub.status.idle": "2023-06-17T15:13:32.999277Z",
     "shell.execute_reply": "2023-06-17T15:13:32.998084Z"
    },
    "papermill": {
     "duration": 0.014403,
     "end_time": "2023-06-17T15:13:33.001745",
     "exception": false,
     "start_time": "2023-06-17T15:13:32.987342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a simplified wiki page document\n",
    "wiki_file = \"/kaggle/input/wikidata/wikipedia-first.txt\"\n",
    "# some gold samples for validation\n",
    "gold_file = \"/kaggle/input/type-facts-in-wikipedia/gold-standard-sample.tsv\"\n",
    "# predicted results generated by your\n",
    "# you are supposed to submit this file\n",
    "result_file = \"/kaggle/working/results.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289b74d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T15:13:33.013243Z",
     "iopub.status.busy": "2023-06-17T15:13:33.012386Z",
     "iopub.status.idle": "2023-06-17T15:13:35.424878Z",
     "shell.execute_reply": "2023-06-17T15:13:35.423604Z"
    },
    "papermill": {
     "duration": 2.421722,
     "end_time": "2023-06-17T15:13:35.427776",
     "exception": false,
     "start_time": "2023-06-17T15:13:33.006054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We import\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22fbc172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T15:13:35.438709Z",
     "iopub.status.busy": "2023-06-17T15:13:35.438313Z",
     "iopub.status.idle": "2023-06-17T15:13:35.450938Z",
     "shell.execute_reply": "2023-06-17T15:13:35.449583Z"
    },
    "papermill": {
     "duration": 0.021312,
     "end_time": "2023-06-17T15:13:35.453674",
     "exception": false,
     "start_time": "2023-06-17T15:13:35.432362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_type(wiki_page):\n",
    "    '''\n",
    "    :param wiki_page is an object contains a title and the first sentence from its wiki page.\n",
    "    :return:\n",
    "    '''\n",
    "    title = wiki_page.title\n",
    "    content = wiki_page.content\n",
    "\n",
    "    #We use POS-tagging\n",
    "    splitted_content = content.split()\n",
    "    POS_tagging = dict(nltk.pos_tag(splitted_content))\n",
    "    \n",
    "    ##### We do two types of matching : considering verbal sentences or non verbal sentences. We compile the implemented groups using re. Considering also non verbal sentences allows us to go from an accuracy of 0.65555.. to an accuracy of 0.68888..\n",
    "    # First : Verbal sentences\n",
    "    compiled = re.compile('( has| have| is| are| was| were| means| refers to)'\n",
    "                          '( it| its| a| an| the)?'\n",
    "                          '( best| most| very| some| any| less)?'\n",
    "                          '( new| former| huge| important| ancient| famous| certain| common| large| largest| tall| tallest| big| biggest| short| shortest| small| smallest)?'\n",
    "                          '( name given to)?'\n",
    "                          '( type of| kind of| version of| list of| class of| piece of| part of( a| the)?| sort of| set of)?'\n",
    "                          '( first| second| third| 1st| 2nd| 3rd| ([a-z]|[1-9])*th)?( [A-Z][a-z]*)?( [a-z]*-[a-z]*)? ([a-z]+)')\n",
    "    \n",
    "    # The 12 groups are : verbs, determinants, adverbs, adjectives, 'name given to' -> appear often in the given corpus, refering words, numbers, proper nouns (start with a capital letter), composed words and end of sentence.\n",
    "    \n",
    "    # We then define the number of groups and do the matching\n",
    "    number_of_groups = 12\n",
    "    matching = compiled.search(content)\n",
    "\n",
    "    # In case it is a non verbal sentence, there is no matching so we do the same as before but without the first group\n",
    "    if matching is None :\n",
    "      compiled = re.compile('( it| its| a| an| the)'\n",
    "                            '( best| most| very| some| any| less)?'\n",
    "                            '( new| former| huge| important| ancient| famous| certain| common| large| largest| tall| tallest| big| biggest| short| shortest| small| smallest)?'\n",
    "                            '( name given to)?'\n",
    "                            '( type of| kind of| version of| list of| class of| piece of| part of( a| the)?| sort of| set of)?'\n",
    "                            '( first| second| third| 1st| 2nd| 3rd| ([a-z]|[1-9])*th)?( [A-Z][a-z]*)?( [a-z]*-[a-z]*)? ([a-z]+)')\n",
    "      number_of_groups = 11\n",
    "      matching = compiled.search(content)\n",
    "    \n",
    "    # In case a matching have been found, we compute POS Tagging. It is based on nouns ('NN' or 'NNS' for plural nouns), as they are the most frequent terms in the corpus, so we search for them through the groups.\n",
    "    if matching is not None :\n",
    "      declare_groups = matching.group(number_of_groups)\n",
    "      if declare_groups in splitted_content :\n",
    "        index = splitted_content.index(declare_groups)\n",
    "        for ind in range(index+1,len(POS_tagging)) :\n",
    "          if POS_tagging[declare_groups] in ['NN','NNS'] :\n",
    "            break\n",
    "          declare_groups = splitted_content[ind]\n",
    "        ##############\n",
    "        index_1 = index + 1\n",
    "        if index_1 < len(splitted_content) :\n",
    "          if (POS_tagging[declare_groups], POS_tagging[splitted_content[index_1]]) == ('NN',)*2 :\n",
    "            declare_groups = splitted_content[index_1]\n",
    "      return declare_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c125907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T15:13:35.465189Z",
     "iopub.status.busy": "2023-06-17T15:13:35.464062Z",
     "iopub.status.idle": "2023-06-17T15:14:23.163492Z",
     "shell.execute_reply": "2023-06-17T15:14:23.16202Z"
    },
    "papermill": {
     "duration": 47.712342,
     "end_time": "2023-06-17T15:14:23.170703",
     "exception": false,
     "start_time": "2023-06-17T15:13:35.458361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the extracted type for Army is not correct!\n",
      "the extracted type for Seam ripper is not correct!\n",
      "the extracted type for Ziggy Stardust is not correct!\n",
      "the extracted type for Hillary Rodham Clinton is not correct!\n",
      "the extracted type for Edip Yuksel is not correct!\n",
      "the extracted type for AC/DC is not correct!\n",
      "the extracted type for Disney Channel is not correct!\n",
      "the extracted type for North and South is not correct!\n",
      "the extracted type for Loreto Region is not correct!\n",
      "the extracted type for Tropical cyclone is not correct!\n",
      "the extracted type for Pinocchio is not correct!\n",
      "the extracted type for Mollusc is not correct!\n",
      "the extracted type for Isthmus is not correct!\n",
      "the extracted type for John the Apostle is not correct!\n",
      "the extracted type for Medusa (animal) is not correct!\n",
      "the extracted type for Crown-of-thorns starfish is not correct!\n",
      "the extracted type for Vaquita is not correct!\n",
      "the extracted type for Nu metal is not correct!\n",
      "the extracted type for Moat is not correct!\n",
      "the extracted type for Alex Trebek is not correct!\n",
      "the extracted type for Smash (album) is not correct!\n",
      "the extracted type for Artillery is not correct!\n",
      "the extracted type for Cerulean is not correct!\n",
      "the extracted type for Communication Studies is not correct!\n",
      "the extracted type for Piston is not correct!\n",
      "the extracted type for Onyx is not correct!\n",
      "the extracted type for MotorStorm is not correct!\n",
      "the extracted type for Phoenicia is not correct!\n",
      "the accuracy of your results is 0.6888888888888889\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    '''\n",
    "    First, extract types from each sentence in the wiki file\n",
    "    Next, use gold samples to evaluate your model\n",
    "    :return:\n",
    "    '''\n",
    "    with open(result_file, 'w', encoding=\"utf-8\") as output:\n",
    "        for page in Parsy(wiki_file):\n",
    "            typ = extract_type(page)\n",
    "            if typ:\n",
    "                output.write(page.title + \"\\t\" + typ + \"\\n\")\n",
    "\n",
    "    # Evaluate on some gold samples for checking your model\n",
    "    cal_acc(gold_file, result_file)\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d22bc5",
   "metadata": {
    "papermill": {
     "duration": 0.006637,
     "end_time": "2023-06-17T15:14:23.182196",
     "exception": false,
     "start_time": "2023-06-17T15:14:23.175559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b77e3a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T15:14:23.194075Z",
     "iopub.status.busy": "2023-06-17T15:14:23.193632Z",
     "iopub.status.idle": "2023-06-17T15:14:37.938341Z",
     "shell.execute_reply": "2023-06-17T15:14:37.937218Z"
    },
    "papermill": {
     "duration": 14.753925,
     "end_time": "2023-06-17T15:14:37.941203",
     "exception": false,
     "start_time": "2023-06-17T15:14:23.187278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# An example about how to use spacy\n",
    "################# EXECUTE THIS LOCALLY\n",
    "def dependency_parser():\n",
    "    # load model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # execute dependency parse\n",
    "    doc = nlp(\"A city is a place where many people live together.\")\n",
    "\n",
    "    # print out\n",
    "    for token in doc:\n",
    "        print(\n",
    "            '{0}({1}) <-- {2} -- {3}({4})'.format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
    "\n",
    "    # display the tree\n",
    "    # http://localhost:5000/\n",
    "    displacy.serve(doc, style='dep')\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f98db434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T15:14:37.953308Z",
     "iopub.status.busy": "2023-06-17T15:14:37.952506Z",
     "iopub.status.idle": "2023-06-17T15:14:39.380377Z",
     "shell.execute_reply": "2023-06-17T15:14:39.379058Z"
    },
    "papermill": {
     "duration": 1.437209,
     "end_time": "2023-06-17T15:14:39.383448",
     "exception": false,
     "start_time": "2023-06-17T15:14:37.946239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code goes here\n",
    "\n",
    "en_model = spacy.load('en_core_web_sm') # Loading the model\n",
    "forbidden_words = ['class', 'classes',\n",
    "                   'list', 'lists',\n",
    "                   'word', 'words',\n",
    "                   'set', 'sets',\n",
    "                   'name', 'names',\n",
    "                   'version', 'versions',\n",
    "                   'term','terms',\n",
    "                   'kind', 'kinds',\n",
    "                   'type', 'types',\n",
    "                   'part', 'parts'] # List of forbidden words\n",
    "\n",
    "def extract_type(wiki_page):\n",
    "    '''\n",
    "\n",
    "    :param wiki_page is an object contains a title and the first sentence from its wiki page.\n",
    "    :return:\n",
    "    '''\n",
    "    title = wiki_page.title\n",
    "    content = wiki_page.content\n",
    "    model_content = en_model(content) # Loading the content in the model\n",
    "    list_of_tokens = [] # Initialize the list of tokens we keep\n",
    "\n",
    "    for token in model_content :\n",
    "      list_of_tokens.append(token.text)\n",
    "      if token.text not in forbidden_words :\n",
    "        if (\n",
    "            (token.head.tag_ in ['VBP', 'VBZ', 'VBD'] and ((token.tag_ in ['NNS','NNP','NN'] and token.dep_ != 'nsubj') or\n",
    "            (token.tag_ == 'VB' and token.dep_ == 'xcomp'))) or\n",
    "            (token.head.tag_ == 'IN' and token.tag_ in ['NN', 'NNP', 'NNS'] and token.dep_ == 'pobj')\n",
    "        ):\n",
    "          return token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "819ab7f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T15:14:39.396025Z",
     "iopub.status.busy": "2023-06-17T15:14:39.395615Z",
     "iopub.status.idle": "2023-06-17T15:22:38.026303Z",
     "shell.execute_reply": "2023-06-17T15:22:38.025031Z"
    },
    "papermill": {
     "duration": 478.645448,
     "end_time": "2023-06-17T15:22:38.034109",
     "exception": false,
     "start_time": "2023-06-17T15:14:39.388661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the extracted type for President of Russia is not correct!\n",
      "the extracted type for Edip Yuksel is not correct!\n",
      "the extracted type for São Tomé and Príncipe is not correct!\n",
      "the extracted type for Tropical cyclone is not correct!\n",
      "the extracted type for Isthmus is not correct!\n",
      "the extracted type for Medusa (animal) is not correct!\n",
      "the extracted type for Crown-of-thorns starfish is not correct!\n",
      "the extracted type for Great Fire of London is not correct!\n",
      "the extracted type for Nu metal is not correct!\n",
      "the extracted type for Moat is not correct!\n",
      "the extracted type for Alex Trebek is not correct!\n",
      "the extracted type for Cerulean is not correct!\n",
      "the extracted type for Communication Studies is not correct!\n",
      "the extracted type for 2029 is not correct!\n",
      "the extracted type for Petros Duryan is not correct!\n",
      "the extracted type for Hadith is not correct!\n",
      "the extracted type for Magnetosphere is not correct!\n",
      "the extracted type for Phoenicia is not correct!\n",
      "the accuracy of your results is 0.8\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    '''\n",
    "    First, extract types from each sentence in the wiki file\n",
    "    Next, use gold samples to evaluate your model\n",
    "    :return:\n",
    "    '''\n",
    "    with open(result_file, 'w', encoding=\"utf-8\") as output:\n",
    "        for page in Parsy(wiki_file):\n",
    "            typ = extract_type(page)\n",
    "            if typ:\n",
    "                output.write(page.title + \"\\t\" + typ + \"\\n\")\n",
    "\n",
    "    # Evaluate on some gold samples for checking your model\n",
    "    cal_acc(gold_file, result_file)\n",
    "\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 561.073024,
   "end_time": "2023-06-17T15:22:40.844534",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-17T15:13:19.77151",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
